\documentclass[12pt]{book}
\usepackage{amsthm}
\usepackage{libertine}
\usepackage{newtxmath}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{multicol}
\usepackage[shortlabels]{enumitem}
\usepackage{siunitx}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{mathrsfs}
\usepackage{accents}
\usepackage{fancyhdr}

\fancyhf{}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}
\fancyhead[C]{\rightmark}
\pagestyle{fancy}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{exercise}{Exercise}[section]
\newtheorem*{remark}{Remark}
\newtheorem*{remarks}{Remarks}

\newcommand{\dd}[1]{\mathrm{d}#1}
\newcommand{\utilde}[1]{\underaccent{\tilde}{#1}}


\title{
    Linear Systems and Introductory Algebraic Geometry
    
    \begin{Large}
        Interdisciplinary Mathematics -- Volume VIII
    \end{Large}
}
\author{Robert C. Hermann}
\date{1974}

\begin{document}

\maketitle

\section{Preface}

Systems Theory is a good contemporary example of the sort of approach to science that I call Interdisciplinary Mathematics.
It arose in the 1940s and 1950s from work in Electrical and Mechanical Engineering, particularly circuit, communication, and optimal control theory.
It offers a unified mathematical framework to look at (and solve) problems in a wide variety of fields.
Further, this mathematics is not of the traditional sort involved in engineering education but involves virtually every field of modern mathematics.

The attraction for me in this subject is that one sees here an interplay of intuition suggested by interesting applications and the fun force and beauty of modern mathematics, as one used to see in the classical period of mathematical physics.
It would be an interesting historical question to ask why mathematical physics no longer plays a vanguard role.
I suspect it is not intrinsic to the subject (as I tried to show in my earlier books, physics, particularly elementary particle physics, is still extremely rich in generating mathematical ideas and intuition), but is due to certain perverse features of the discipline.
Many leading physicists are smug; a manifestation of this is the poor education of physics students in mathematics, as contrasted say with that of the best engineering students.
Not that physicists don't learn mathematics - my criticism is that they mainly learn outdated or overly `pragmatic' mathematics.
On the other hand, what exists in the way of organized mathematical physics today (typified, say, by the Communications in Mathematical Physics or Journal of Mathematical Physics) carries over the vices of aridity and sterility of contemporary mathematics.
Without sufficiently close contact with the problems arising in mainstream physics, it is often an intellectual stream of physics, made much worse by the rigidity enforced on the scientific world in the last few years by financial troubles.

My interest in Systems Theory is primarily in its mathematical structure, particularly involving algebra and geometry, and in its applications to areas outside of engineering, e.g., in the books by Brockett, Desoer, and Kalman-Arbib-Falb, but done in a way that I believe will be interesting even to experts, since I use Linear Algebra more systematically and efficiently than before. This material would be an excellent topic for a senior or beginning graduate course in mathematics or engineering. I have added expository material on Algebraic Geometry, since, as R. Kalman has emphasized, this area of mathematics is highly relevant to the ultimate development of Systems Theory. Volume IX will develop the ramifications into differential geometry, which is much closer to my own research interest. I will occasionally refer to my other books. ``Volume II'' refers, for example, to the second volume of this interdisciplinary series. The other books are referred to by abbreviations listed in the bibliography. For example, ``LGP'' refers to ``Lie Groups for Physicists.''

Over the years, I have benefited in a general way from conversations on Systems Theory with R. Brockett and R. Kalman, and I would like to thank them. Prof. John Stachel has provided excellent scientific hospitality, inspiration, and working conditions for me as a visitor at the Institute for Relativity Studies of Boston University, and I thank him. Alta Zapf has again typed magnificently.

\chapter{General Ideas of Linear Systems Theory}

\section{Introduction}

The main aim of this volume is the study of the algebraic ideas that enter into the study of linear, time-invariant, finite-dimensional systems.
(Refer to Brockett [1], Desoer [1], Kalman, Arbib and Falb [1], Rosenbrock [1], Zadeh and Desoer [1] for the standard theory.) %TODO: Add reference in the bibliography and don't hard code ref numbers.
In fact, R. Kalman has been the systems theorist who has consistently suggested a program of study of linear systems by means of the tools of algebra, and here I intend to follow his suggestion.

Because it is closest to my own intuition, I shall begin with a differential equation framework for system theory, which of course means that we work over the real or complex numbers as ground field.
However, there are various means of generalization available (e.g. replacing the differential equations by difference equations) that enables one to build systems that work with other fields as scalars.
Additionally, much of automata and coding theory involves similar algebraic ideas, with fields of finite characteristic appearing as the scalar field.

\section{A Definition of a ``System'' in Terms of Differential Equations}

Let $K$ denote the real or complex numbers.
Suppose given three vector spaces (always finite dimensional) over $K$, denoted by
$$U, X \text{ and } Y,$$
with:
\begin{itemize}
    \item $U$ called the \textbf{input space}.
    \item $Y$ called the \textbf{output space}.
    \item $X$ called the \textbf{state space}.
\end{itemize}
Also, let $f, g$ be maps with the following domains and ranges,
\begin{align}
    f: X \times U &\to X \\
    g: X \times U &\to Y.
\end{align}
With this choice of spaces and maps we associate the following system of ordinary differential equations:
\begin{align}
\begin{split} \label{eq:ODESystem1}
    \frac{\dd{x}}{\dd{t}} &= f(x, u) \\
    y(t) &= g(x(t), u(t)).
\end{split}
\end{align}
The ordered 5-tuple
$$\sigma = (U, X, Y, f, g)$$
is said to be a \textbf{system}.

Typical elements of $X, U, Y$ are denoted $x, u, y$. $t$ is a real variable, ranging say over
$$0 \leq t < \infty.$$
A curve
$$t \mapsto (x(t), u(t), y(t)) \in X \times U \times Y$$
is said to be a \textbf{trajectory} of the system if it is a solution of the equations (\ref{eq:ODESystem1}).
The curve $t \mapsto u(t)$ in $U$ is then called the \textbf{input}, the curve $t \mapsto x(t)$ is called the \textbf{state curve}, and the curve $t \mapsto y(t)$ is called the \textbf{output}.

\begin{definition}
    A curve $t \mapsto (u(t), y(t))$ in $U \times Y$ is called an \textbf{input-output pair} if there is a curve $t \mapsto x(t)$ in the state space $X$ such that
    $$t \mapsto (x(t), u(t), y(t))$$
    is a solution of (\ref{eq:ODESystem1}), i.e. is a trajectory of the system.
\end{definition}

One main mathematical goal of systems theory is to study to what extent the set of input-output pairs characterizes the whole system.

\begin{remarks}
\begin{enumerate}
    \item I have chosen this specific mathematical framework for describing what is meant by a "system" so that we can go to work with a minimum of definitions.
    Refer to Kalman, Arbib and Falb [1], Zadeh and Desoer [1], for the full story. % TODO: Move refs to bibliography.
    Of course, one possible generalization is to allow the functions $f( , ), g( , )$ on the right hand side of (\ref{eq:ODESystem1}) to depend on $t$.
    Another generalization would be to allow $X, Y, U$ to be manifolds.
    Such a formulation of systems theory is being developed by Sussman [1]. % TODO: Move refs to bibliography.
    I hope to consider it in another part of this treatise.
    \item It is, of course, useful if the reader is familiar with the areas of application from which this mathematical version of ``systems theory'' arose.
    One such area is electrical circuit theory.
    The book by Anderson and Vongpanitlerd [1] is recommended here. % TODO: Move refs to bibliography.
    There has been widespread discussion in the relevant literature about the possibilities of applying ``systems theoretic'' ideas, of the sort to be described here, to computer science, biology, economics, etc., but I do not know very much about this work, hence will not comment on it.
    Of course, systems theory is very closely related to optimal control theory, and there are many important applications via this connection to economics, mechanical engineering, operations research, etc.
    \item One will find many ways in the engineering literature to describe ``systems'' by means of diagrams.
    Of course, a pictorial representation suggests itself from the ``input'' and ``output'' for the curves $t \mapsto u(t), y(t)$.

    \begin{equation*}
    \begin{tikzpicture}
        \node[draw, rectangle, minimum width=2cm, minimum height=1cm] (box) at (0,0) {};
        \draw (box.west) -- ++(-2,0) node[above]{$u$} node[below]{input};
        \draw (box.east) -- ++(2,0) node[above]{$y$} node[below]{output};
    \end{tikzpicture}
    \end{equation*}
    
    In this way of thinking, the ``state'' structure is thought of as a ``black box,'' which is known only via its response to various inputs.
    Here we touch on a very interesting difference in outlook between physics and engineering.
    From the mathematical point of view, much of physics is the study of the pair $(X, \varphi)$ consisting of a ``state space'' $X$, and a one-parameter semi-group of transformations $t \mapsto \varphi(t): X \to X$ acting on $X$.
    If the initial state $x_0 \in X$ is given at $t = 0$, the future
    $$t \mapsto \varphi(t)(x_0)$$
    is known.
    This outlook is clearest, of course, in classical Newtonian mechanics, where $X$ is finite dimensional and $\varphi$ is determined by solving ordinary differential equations, but it carries over also to quantum mechanics, at the expense of making $X$ and $\varphi$ more complicated mathematically.
    This framework suggests the physicist's main job is to ``approximate'' $X$ as best he can, and make predictions to be checked with experiment.
    This implies that there is some physical ``truth'' or ``law'' that the scientist is trying to discover.
\end{enumerate}
\end{remarks}

Let us call the pair $(X, \varphi)$ the \textbf{physicists' dynamical system}.
We can see a difference in outlook between the physicist and engineer by examining the difference between $(X, \varphi)$, and a ``system''
$$(X, Y, U, f, g).$$
Mathematically, we compare
\begin{align} \label{eq:PhysDynSys1}
    \frac{\dd{x}}{\dd{t}} &= \varphi(x(t))
\end{align}
and
\begin{align}
\begin{split} \label{eq:ODESystem2}
    \frac{\dd{x}}{\dd{t}} &= f(x(t), u(t)) \\
    y(t) &= g(x(t), u(t)).
\end{split}
\end{align}
Of course, (\ref{eq:ODESystem2}) can be constructed to reduce to (\ref{eq:PhysDynSys1}) as a special case.
However, what is basically different is the outlook -- the input curve $t \mapsto u(t)$ is to be chosen so as to achieve \textbf{some desired properties of the output curve} $t \mapsto y(t)$.
Discovering the precise ``nature'' of the state-mechanism, i.e., the functions $f$ and $g$, is not so important as in physics.
One then gives up the deeply satisfying feeling one gets from studying physics that one is trying to discover deep ``truths,'' and instead puts the emphasis on desired ends.
One might say then that ``systems theory'' is the right mathematics for technology and social phenomena.
I am not sufficiently learned in the philosophy of science to go into these distinctions in any greater depth or detail, but it is clear in my mind that it would be worthwhile to do so -- in fact, it might even give us vastly expanded insights into the times we live in.

Returning to the mathematics of systems theory, we shall, for the rest of the chapter at least, deal with the only sort that is completely mathematically tractable at the moment, the linear, time-invariant ones.
This means that $f, g$, considered as maps
\begin{align*}
\begin{split}
    f: X \times U &\to X \\
    g: X \times U &\to Y
\end{split}
\end{align*}
between vector spaces, are \textbf{linear} maps.
($X \times U$ is considered to be the direct sum of the vector spaces $X$ and $U$). Thus, $f$ and $g$ are of the following form:
\begin{align}
\begin{split}
    f(x, u) &= Ax + Bu \\
    g(x, u) &= Cx + Du,
\end{split}
\end{align}
where $A: X \to X, B: U \to X, C: X \to Y, D: U \to Y$ are linear maps.
Such a system can then be characterized by
$$(X, U, Y, A, B, C, D).$$
We shall develop the algebraic tools needed to understand the ``structure'' of these linear systems.
They mainly involve linear algebra of a more-or-less standard sort, but -- as R. Kalman emphasizes -- lead one also into problems which are closely related to current pure mathematics research in algebraic geometry and algebraic group theory.

\section{Equivalent and Linearly Equivalent Linear, Time-Invariant Systems}

Let us recapitulate the notation.
$K$ is the field of either the real or complex numbers.
(I shall sketch in a moment how replacing ``differential'' by ``difference'' equations enables one to replace $K$ by more general scalar fields.)
$X, U, Y$ are vector spaces,
\begin{align*}
\begin{split}
    A: X &\to X \\
    B: U &\to X \\
    C: X &\to Y \\
    D: U &\to Y
\end{split}
\end{align*}
are linear maps.
Denote the whole collection by:
\begin{align} \label{eq:sigma1}
    \sigma &= (U, X, Y, A, B, C, D).
\end{align}
We shall call such a $\sigma$ a \textbf{linear system}. The set of all such $\sigma$, with given $U, X, Y$, is denoted by
$$\Sigma.$$
All the data necessary to define a linear system may be summarized in the following diagram of vector spaces and linear maps.
(Note that it is not a ``commutative'' diagram.)

\begin{equation*}
\begin{tikzcd}
    U \ar[r,"D"] \ar[d,"B"'] & Y \\
    X \ar[r,"A"'] & X \ar[u,"C"']
\end{tikzcd}
\end{equation*}

With $\sigma$ given by (\ref{eq:sigma1}), we can consider the following system of linear constant-coefficient equations:
\begin{align}
\begin{split} \label{eq:LinearODESystem1}
    \frac{\dd{x}}{\dd{t}}(t) &= Ax(t) + Bu(t) \\
    y(t) &= Cx(t) + Du(t).
\end{split}
\end{align}

Here is the terminology that goes along with these equations.
$X$ is called the \textbf{state space}.
A typical element is denoted by $x$.

$U$ is called the \textbf{input space}.
A typical element of $U$ is denoted by $u$.
A curve $\utilde{u}: t \mapsto u(t), -\infty < t < \infty,$ in $U$ is called an \textbf{input curve}.

$Y$ is called the \textbf{output space}.
A typical element is denoted by $y$.
A curve $\utilde{y}: t \mapsto y(t)$ is called an \textbf{output curve}.

$\utilde{U}$ and $\utilde{Y}$ denote the space of input and output curves.

\begin{definition}
    An element $(\utilde{u}, \utilde{y}) \in \utilde{U} \times \utilde{Y}$ is called an \textbf{input-output pair} if there is a curve $t \mapsto x(t)$ in the state space $X$, such that the curves $t \mapsto (x(t), u(t), y(t))$ satisfy equation (\ref{eq:LinearODESystem1}).
\end{definition}

Two linear systems $\sigma, \sigma'$ are said to be \textbf{equivalent} if they have the same set of input-output pairs.

\begin{remark}
    To replace (\ref{eq:LinearODESystem1}) by difference equations, let $t$ denote an element of the additive semigroup $\mathbb{Z}_+$, the non-negative integers.
    An input or output curve is now a map
    \begin{align*}
    \begin{split}
        t \mapsto u(t) &\text{ of } \mathbb{Z}_+ \to U \\
        t \mapsto y(t) &\text{ of } \mathbb{Z}_+ \to U.
    \end{split}
    \end{align*}
    Replace $\frac{\dd{u}}{\dd{t}}$ by
    $$\Delta u(t) = u(t + 1) - u(t).$$
    Thus, (\ref{eq:LinearODESystem1}) may be interpreted as a set of difference equations:
    \begin{align*}
    \begin{split}
        \Delta u(t) &= Ax(t) + Bu(t) \\
        y(t) &= Cx(t) + Du(t).
    \end{split}
    \end{align*}
    A further generalization might involve putting a ``delay'' in the differential or difference equations.
    However, we shall work with the differential equation set-up, as the most familiar (at least to me).
\end{remark}

We shall now discuss one concrete way of realizing equivalence of two linear systems with the same $X, U, Y$.
Let $\mathrm{GL}(X)$ denote the space of invertible linear maps
$$g: X \to X.$$
Given $t \mapsto (x(t), u(t), y(t))$ which is a solution of (\ref{eq:LinearODESystem1}), set:
$$x_1(t) = gx(t).$$
Then,
\begin{align}
\begin{split} \label{eq:DifferenceEquationSystem1dx/dt}
    \frac{\dd{x_1}}{\dd{t}} = g\frac{\dd{x}}{\dd{t}} &= g(Ax + Bu) \\
    &= A_1x_1 + B_1u,
\end{split}
\end{align}
with:
\begin{align}
    A_1 &= gAg^{-1} \\
    B_1 &= gB.
\end{align}
Also,
\begin{align}
\begin{split} \label{eq:DifferenceEquationSystem1y}
    y(t) &= Cg^{-1}x_1 + Du \\
    &= C_1x_1 + D_1u
\end{split}
\end{align}
with:
\begin{align}
    C_1 &= Cg^{-1}, \label{eq:C1,1} \\
    D_1 &= D.
\end{align}

We see that equations (\ref{eq:DifferenceEquationSystem1dx/dt}) and (\ref{eq:DifferenceEquationSystem1y}) define a new linear time invariant system
$$\sigma_1 = (U, X, Y, A_1, B_1, C_1, D_1).$$
We see that $\sigma$ and $\sigma_1$ automatically have the same set of input-output pairs, i.e. define equivalent systems.

Let us formalize this construction.

\begin{definition}
    Let $\sigma = (A, B, C, D)$ and $\sigma_1 = (A_1, B_1, C_1, D_1)$ be two linear systems with the same state, input, and output space.
    They are said to be \textbf{algebraically equivalent} if and only if there is a
    $$g \in \textrm{GL}(V)$$
    such that:
    \begin{align}
    \begin{split}
        A_1 &= gAg^{-1} \\
        B_1 &= gB \\
        C_1 &= Cg^{-1} \\
        D_1 &= D.
    \end{split}
    \end{align}
\end{definition}

Our discussion above proves the following (trivial) result:
\begin{theorem}
    If $\sigma = (A, B, C, D)$ and $\sigma_1 = (A_1, B_1, C_1, D_1)$ are algebraically equivalent, then they are equivalent.
\end{theorem}

\begin{remark}
    Sufficient conditions that ``equivalence'' implies ``algebraic equivalence'' are discussed by Brockett [1] and Desoer [1]. % TODO: Add actual citations to bibliography.
    Roughly if the systems are ``minimal'' (in the sense of having a minimal dimensional state space) then the two notions coincide.
    We shall deal with this later.
\end{remark}

We can now reformulate ``algebraic equivalence'' in a more standard group-theoretic way.
Let $\Sigma = \{\sigma\}$ denote the collection of linear systems
$$\sigma = (A, B, C, D).$$
In other work, % TODO: Is this a typo?
\begin{align} \label{eq:Sigma1}
    \Sigma &= L(X, X) \times L(U, X) \times L(X, Y) \times L(U, Y).
\end{align}

\begin{remark}
    In (\ref{eq:Sigma1}), I use the notation from linear algebra, e.g. from volume II. If $V, W$ are vector spaces,
    $$L(V, W)$$
    denotes the vector space of linear maps $V \to W$.
\end{remark}

Let $\mathrm{GL}(X)$ be the group of invertible linear maps $X \to X$.
Let $\mathrm{GL}(X)$ act on $\Sigma$ as follows:

If $g \in \mathrm{GL}(X)$, and
$$\sigma = (A, B, C, D) \in \Sigma,$$
then
\begin{align} \label{eq:gsigma1}
    g\sigma &= (gAg^{-1}, gB, Cg, D).
\end{align}

\begin{exercise}
    Show that (\ref{eq:gsigma1}) defines an action of $\mathrm{GL}(X)$ on $\Sigma$ as a transformation group.
\end{exercise}

Let us restate things in the following way.

\begin{theorem}
    Two linear systems $\sigma, \sigma' \in \Sigma$ are algebraically equivalent if and only if they are on the same orbit of $\mathrm{GL}(X)$ acting on $\Sigma$ via the transformation group action (\ref{eq:gsigma1}).
    Hence, the equivalence classes of linear systems under the equivalence relation defined by ``algebraic equivalence'' are in one-one correspondence with the orbit space
    \begin{align} \label{eq:OrbitSpace1}
        \mathrm{GL}(X)\backslash\Sigma \equiv \{\mathrm{GL}(X)\sigma : \sigma \in \Sigma\}.
    \end{align}
\end{theorem}

We conclude from these remarks that the study of the geometric and algebraic properties of the orbit space (\ref{eq:OrbitSpace1}) might be in close rapport with the properties of elements of $\Sigma$ that are interesting from an engineering point of view.
In this chapter we shall pursue these interconnections.

\begin{remark}
    Notice that the $D$ component of the system $\sigma = (A, B, C, D)$ plays no role in algebraic equivalence, hence it can usually be left out, i.e. be set equal to zero.
\end{remark}

\section{Impulse Response and Transfer Functions; Observability and Controllability}

The differential equations governing linear, time-invariant, ``lumped parameter'' systems, i.e. those of form (\ref{eq:LinearODESystem1}), are relatively easy to handle.
In fact, an explicit solution can be written down for them.
These explicit solutions involve what are called ``impulse responses'' and ``transfer functions'' in the engineering literature.
(Brockett [1] follows terminology introduced by Kalman and gives the name ``weighting pattern'' to what we call ``impulse response.'') % TODO: Add ref to bibliography.

Suppose $\sigma = (A, B, C, D)$ is a system, whose input-output response is governed by the equations (\ref{eq:LinearODESystem1}).
We can, of course solve equations (\ref{eq:C1,1}) explicitly by means of the variation-of-parameter technique and the operator exponential
\begin{align}
    e^{tA} &= 1 + tA + \frac{t^2A}{2} + \dots
\end{align}
(see Vol. III).

Here are the equations:
\begin{align}
    \frac{\dd{x}}{\dd{t}} &= Ax + Bu \label{eq:LinearODESystemdxdt1} \\
    y &= Cx + Du. \label{eq:LinearODESystemy1}
\end{align}
The solution to (\ref{eq:LinearODESystemdxdt1}) is:
\begin{align}
    x(t) &= \int_0^t e^{A(t-s)}Bu(s)\ \dd{s} + e^{At}x(0).
\end{align}
Hence, the complete solution to the system (\ref{eq:LinearODESystemy1}) is:
\begin{align} \label{eq:LinearODESystemy1Solution1}
    y &= \int_0^t Ce^{A(t-s)}Bu(s)\ \dd{s} + Ce^{At}x(0) + Du.
\end{align}

\begin{definition}
    The function
    \begin{align} \label{eq:ImpulseResponse1}
        t &\mapsto Ce^{At}B: R \to L(U, Y)
    \end{align}
    is called the \textbf{impulse response} for the system $\sigma = (A, B, C, D)$.
    Its Laplace transform
    \begin{align} \label{eq:FreqResponse1}
        s &\mapsto C(a-s)^{-1}B
    \end{align}
    is called the \textbf{frequency response} of the system.
\end{definition}

\begin{remarks}
\begin{enumerate}
    \item \textbf{Resolvents}.
    In functional analysis, the operator-valued function
    $$s \mapsto (A-s)^{-1}$$
    is called the \textbf{resolvent} of the operator $A$.
    See Dunford and Schwartz [1], Kato [1], Yosida [1], Reed and Simon [1]. % TODO: Add refs to bibliography.
    Its theory, for operators in infinite dimensional space, plays a big role in mathematical physics and Lie group harmonic analysis.
    See LMP, vol. II, and PALG.
    Of course, the traditional problems of control theory involve a finite dimensional state space $X$ (or at least this is the only situation for which a complete theory exists), but presumably a pursuit of the ultimate ramification and application of systems theory will eventually involve infinite dimensional situations in a critical way.
    \item \textbf{Rational mappings}.
    Since one of our aims is to develop the algebro-geometric foundations of systems theory, it is interesting to ask what kind of an abstract ``object'' the transfer function, given by (\ref{eq:FreqResponse1}), actually is.
    Of course, we can regard it as a mapping in the ordinary sense, by deleting from our ground field, (in this case, the complex numbers $\mathbb{C}$) the eigenvalues of $A$, and regarding (\ref{eq:FreqResponse1}) as a mapping:
    $$C - (\text{eigenvalues of } A) \to L(U, Y).$$
    I do not regard this as a completely satisfactory answer, since it says nothing very precise about what happens at the singular points of the mapping, i.e. the eigenvalues of $A$.
    Algebraic geometry provides us with a ``language'' to discuss (\ref{eq:FreqResponse1}), as a ``rational'' mapping
    $$C \to L(U, Y).$$
    Indeed, this may be the chief reason for introducing algebra-geometric ideas into systems theory.
    \item \textbf{Generalizations to other scalar fields}.
    As (\ref{eq:ImpulseResponse1}) shows, the impulse response does not involve ``algebraic'' functions, while the transfer function does.
    Thus, discussing algebraic objects like the transfer function over other scalar fields than the complex numbers might be a useful generalization.
    For example, it might be relevant to situations where the differential equations were replaced by difference equations.
\end{enumerate}
\end{remarks}

Here is a main result.

\begin{theorem} \label{thm:Equivalence1}
    Let $\sigma = (U, X, Y, A, B, C, D), \sigma_1 = (U, X_1, Y, A_1, B_1, C_1, D_1)$ be two linear systems which have the same input and output vector space, $U$ and $Y$, but possibly different state spaces $X$ and $X_1$.
    Suppose that they are equivalent, in the sense of having the same input and output pairs.
    Then,
    \begin{align}
        C(A-s)^{-1}B &= C_1(A_1-s)^{-1}B_1, \forall s \in \mathbb{C},
    \end{align}
    i.e. they have the same frequency responses.
    Also,
    \begin{align}
        D &= D_1.
    \end{align}
\end{theorem}

\begin{proof}
    Let $t \mapsto u(t)$ be an input curve, and let $x(0)$ be an initial state vector. Let $t \mapsto y(t)$ be the output curve determined by $\{t \mapsto u(t), x(0)\}$ in system $\sigma$. Using (\ref{eq:LinearODESystemy1Solution1}), $y(t)$ is determined by the following formula:
    \begin{align} \label{eq:LinearODESystemy1Solution2}
        y(t) &= \int_0^t Ce^{A(t-s)}Bu(s)\ \dd{s} + Ce^{At}x(0) + Du(t).
    \end{align}
    
    By hypothesis, $\sigma$ and $\sigma_1$ are equivalent.
    This means that there must be a state vector $x_1(0) \in X_1$, such that $\{t \mapsto u(t), x_1(0)\}$ determines the output $t \mapsto y(t)$ in the $c_1$ system, i.e. that:
    \begin{align} \label{eq:LinearODESystemy1Solution3}
        y(t) &= \int_0^t C_1e^{A_1(t-s)}B_1u(s)\ \dd{s} + C_1e^{A_1t}x_1(0) + D_1u(t).
    \end{align}
    Equating (\ref{eq:LinearODESystemy1Solution2}) and (\ref{eq:LinearODESystemy1Solution3}), we have:
    \begin{align} \label{eq:LinearODESystemy1Solution4}
        0 &= \int_0^t \left[Ce^{A(t-s)}B-C_1e^{A_1(t-s)}B_1\right]u(s)\ \dd{s} + Ce^{A_0t}x(0) - C_1e^{A_1t}x_1(0) + (D-D_1)u(t).
    \end{align}
    
    Let us recapitulate the logic which led to equation (\ref{eq:LinearODESystemy1Solution4}).
    The input curve $t \mapsto u(t)$, in $U$ and the state vector $x(0) \in X$ is given.
    $x_1(0)$ is a vector in $X_1$ such that (\ref{eq:LinearODESystemy1Solution4}) holds for all $t$.
    Let us now put in special inputs and states.
    For example:
    \begin{align}
    \begin{split} \label{eq:SpecialInputsStates1}
        u(t) &= e^{-\lambda t}u(0) \\
        x(0) &= 0,
    \end{split}
    \end{align}
    where $\lambda$ is a complex number, $u(0)$ an arbitrary element of $U$.
    Now,
    \begin{align}
    \begin{split} \label{eq:Integral1}
        \int_0^t e^{A(t-s)}e^{-s}\ \dd{s} &= e^{At}\int_0^t e^{-s(A+\lambda)}\ \dd{s} \\
        &= \left.e^{At}(-1)(A+\lambda)^{-1}e^{-s(A+\lambda)}\right|_{s=0}^t \\
        &= -e^{At}(A+\lambda)^{-1}(e^{-t(A+\lambda)} - 1) \\
        &= e^{At}(A+\lambda)^{-1}(e^{At}-e^{-\lambda t}).
    \end{split}
    \end{align}
    Substituting (\ref{eq:LinearODESystemy1Solution4}) into (\ref{eq:SpecialInputsStates1}), and using (\ref{eq:Integral1}) we have:
    \begin{align}
    \begin{split}
        0 &= C(A+\lambda)^{-1}(e^{At}-e^{-\lambda t})Bu(0) - C_1(A_1+\lambda)^{-1}(e^{A_1t}-e^{-\lambda t})B_1u(0) \\
        &+ C_1e^{A_1t}x_1(0) + (D-D_1)u(0).
    \end{split}
    \end{align}
    We see that the coefficients of $e^{-\lambda t}$ in relation (\ref{eq:SpecialInputsStates1}) must vanish, hence:
    \begin{align}
        C(A+\lambda)^{-1}B &= C_1(A_1+\lambda)^{-1}B_1.
    \end{align}
    This says that the frequency response functions are equal, hence proves Theorem \ref{thm:Equivalence1}.
\end{proof}

\begin{remark}
    We can interpret this result in the following way.
    Let $\Sigma$ be the space of linear systems with given input and output spaces $U, Y$.
    Let $E$ be the equivalence relation on $\Sigma$ defined by saying that two systems are the same if they have the same set of input-output pairs.
    Let
    $$\textrm{RM}(C, L(U, Y))$$
    be the space of rational maps: $C \to L(U, Y)$.
    (Later on, after presenting the relevant ideas from algebraic geometry, we shall define $\textrm{RM}$ precisely.)
    Thus, the ``frequency response'' construction defines a map
    \begin{align} \label{def:FrequencyResponseMap1}
        \textrm{FR}: \Sigma &\to \textrm{RM}(C, L(U, Y)).
    \end{align}
    Theorem \ref{thm:Equivalence1} says that this map is constant on the equivalence classes of $E$, hence the map (\ref{def:FrequencyResponseMap1}) passes to the quotient to define a map
    \begin{align}
        \textrm{FR}: E\backslash\Sigma \to \textrm{RM}(C, L(U, Y)).
    \end{align}
    The properties of this map will be one of our main concerns.
\end{remark}

Now we turn to the discussion of the type of system -- the ``completely controllable and observable'' ones -- for which equivalence implies algebraic equivalence.

\section{Completely Controllable and Observable Systems}
Let $\sigma$ be a linear system, described by the following diagram of spaces and mappings

\begin{equation}
\begin{tikzcd} \label{diag:LinearSystem1}
    U \ar[r,"D"] \ar[d,"B"'] & Y \\
    X \ar[r,"A"'] & X \ar[u,"C"']
\end{tikzcd}
\end{equation}

\begin{definition}
    $\sigma$ is said to be \textbf{completely controllable} if the following condition is satisfied:
    \begin{align}
        X &= B(U) + AB(U) + A^B(u) + \dots
    \end{align}
    $\sigma$ is said to be \textbf{completely observable} if the following condition is satisfied.
    \begin{align} \label{eq:CompletelyObservableCondition1}
        \text{Given non-zero } x \in X, \text{ there is an integer } n \text{ such that } \textrm{CA}^n(x) &\neq 0.
    \end{align}
    Alternatively, one may say that:
    \begin{align} \label{eq:CompletelyObservableCondition2}
        \ker(C) \cap \ker(CA) \cap \dots \cap  \ker(CA^n) \cap \dots &= 0.
    \end{align}
\end{definition}

\begin{remark}
    The practical and theoretical meaning of conditions () and () is extensively discussed in the treatises, e.g. Brockett [1], Desoer [1] and Kalman, Arbib, and Falb [1]. % TODO: Add refs in bibliography
    Recall that (\ref{diag:LinearSystem1}) means that, given $x_0, x_1 \in X$, an input curve $t \mapsto u(t), a \leq t \leq b$ can be chosen so that the solution of the equation:
    $$\frac{\dd{x}}{\dd{t}} = Ax + Bu(t)$$
    goes from $x_0$ to $x_1$ as $t$ varies from $a$ to $b$.
    In volume III I have given a brief ``functional analysis'' proof of this fact.
    A differential geometric proof (which I regard as more profound because it extends to non-linear systems) is presented in DGCV.
    As we shall see later on, ``complete observability'' is ``dual'' to controllability.
\end{remark}

Here is a main result.

\begin{theorem} \label{thm:Equivalence2}
    Let $\sigma = (A, B, C, D), \sigma_1 = (A_1, B_1, C_1, D_1)$ be completely observable linear systems, with the same input, output and state spaces.
    Suppose $\sigma$ and $\sigma_1$ are equivalent. Then, $\sigma$ and $\sigma_1$ are algebraically equivalent.
\end{theorem}

\begin{proof}
    Let $t \mapsto u(t), y(t)$ be an input-output curve for the system $\sigma$.
    By (\ref{eq:LinearODESystemy1Solution2}), this means that there is an $x_0 \in X$ such that
    \begin{align} \label{eq:CompletelyObservableLinearODESystemy1}
        y(t) &= \int_0^t Ce^{A(t-s)}Bu(s)\ \dd{s} + Ce^{At}x_0 + Du(t).
    \end{align}
    To say that $\sigma$ is equivalent to $\sigma_1$ is to say that $t \mapsto (u(t), y(t))$ is also an input-output curve for the system $\sigma_1$, i.e. that there is an $x_1 \in X$ such that:
    \begin{align} \label{eq:CompletelyObservableLinearODESystemy2}
        y(t) &= \int_0^t C_1e^{A_1(t-s)}Bu(s)\ \dd{s} + C_1e^{A_1t}x_1 + D_1u(t).
    \end{align}
    Now, by Theorem \ref{thm:Equivalence1}, $\sigma$ and $\sigma_1$ have the same frequency response function.
    Hence, they have the same impulse response function (since ``frequency response'' is the Laplace transform of ``impulse response'', and the Laplace transform is a one-one map), i.e. the first terms on the right hand sides of (\ref{eq:CompletelyObservableLinearODESystemy1}) and (\ref{eq:CompletelyObservableLinearODESystemy2}) are equal.
    Hence, we have:
    \begin{align}
        Ce^{At}x_0 + Du(t) &= C_1e^{A_1t}x_1 + D_1u(t).
    \end{align}
    Now, $t \mapsto u(t)$ may be an \emph{arbitrary} curve in the input space $U$. $(D-D_1)$ is a linear map
    $$U \to Y.$$
    It should be intuitively clear that the image of an arbitrary curve in $U$ under the linear map $D-D_1$ is the sum of exponentials if and only if $D-D_1=0$, i.e.
    \begin{align} \label{eq:D=D_1}
        D &= D_1.
    \end{align}
\end{proof}

\begin{exercise}
    Give the details of the proof of (\ref{eq:D=D_1}).
\end{exercise}

With (\ref{eq:D=D_1}) satisfied, everything reduces to the following identity:
\begin{align} \label{eq:Ce^(At)x_0=C_1e^(A_1t)x_1}
    Ce^{At}x_0 &= C_1e^{A_1t}x_1.
\end{align}
Using the power-series expansion of $e^{At}$, (\ref{eq:Ce^(At)x_0=C_1e^(A_1t)x_1}) implies the following series of algebraic identities:
\begin{align} \label{eq:AlgebraicIdentities1}
    CA^n(x_0) &= C_1A_1^n(x_1) \text{ for } n = 0, 1, \dots
\end{align}
Now, change the logic somewhat, and suppose the input curve $t \mapsto u(t)$ and the state vector $x_0 \in X$ are given.
Define the output $t \mapsto y(t)$ of $\sigma$ by formula (\ref{eq:CompletelyObservableLinearODESystemy1}).
Here is the diagram for this:

\begin{equation}
\begin{tikzpicture} \label{diag:x_0}
    \node[draw, rectangle, minimum width=1cm, minimum height=1cm] (box) at (0,0) {};
    \node[above] (x0) at (box.north) {$x_0$};
    \node[above] at (x0.north) {state};
    \draw (box.west) -- ++(-2,0) node[above]{$u$};
    \draw (box.east) -- ++(2,0) node[above]{$y$};
\end{tikzpicture}
\end{equation}

In words, we set the ``dial'' in the ``black-box'' represented by $\square$ so that it is in state $x_0$, and read the output curve $t \mapsto y(t)$.

Now, note that ``complete observability'' of $\sigma_1$ implies that $x_1$ is completely determined by $x_0$ and condition (\ref{eq:AlgebraicIdentities1}).
For, if $x_1^1$ were another element of $X$ such that
$$CA^n(x_0) = C_1A_1^n(x_1^1) \text{ for all } n,$$
then
$$C_1A_1^n(x_1^1-x_1) = 0 \text{ for all } n,$$
which, by (\ref{eq:CompletelyObservableCondition1}), implies
$$x_1^1 = x_1.$$

Let us now run the ``machine'' represented by (\ref{diag:x_0}) with zero input, and given initial state $x_0$.
There is then an $x_1$ such that (\ref{eq:AlgebraicIdentities1}) is satisfied, which is uniquely determined by $x_0$. We can then define a map
$$g: X \to X$$
so that:
\begin{align} \label{eq:g(x_0)=x_1}
    g(x_0) &= x_1.
\end{align}
Since $x_1$ is uniquely determined by $x_0$, and the conditions (\ref{diag:x_0}) (regarded as an equation for $x_1$) are linear, the map $g$ is linear. % TODO: Is this reference a typo?
Thus, $g$ satisfies the following conditions:
\begin{align} \label{eq:CA^n=C_1A_1^ng}
    CA^n &= C_1A_1^ng \text{ for all } n.
\end{align}
In particular,
\begin{align} \label{eq:C=C_1g}
    C &= C_1g.
\end{align}
Complete observability of $\sigma$ implies $g$ is one-one.
For, if
$$gx = 0,$$
(\ref{eq:CA^n=C_1A_1^ng}) is used to imply
$$CA^nx = 0 \text{ for all } n,$$
and (\ref{eq:CompletelyObservableCondition1}) implies $x = 0$.
Finite dimensionality of $X$ and linearity of $g$ now implies that $g^{-1}$ exists.
To prove:
\begin{align} \label{eq:A_1=gAg^(-1)}
    A_1 = gAg^{-1}.
\end{align}

\textbf{Proof of \ref{eq:A_1=gAg^(-1)}}.
Combine (\ref{eq:C=C_1g}) and (\ref{eq:CA^n=C_1A_1^ng})
$$C_1gA^n = C_1A_1^ng,$$
or
\begin{align} \label{eq:C_1(gA^ng^(-1)-A_1^n)=0}
    C_1(gA^ng^{-1}-A_1^n) &= 0 \text{ for all } n.
\end{align}
Now, for $n = 1$,
$$C_1(gAg^{-1} - A_1^n) = 0.$$
Hence,
\begin{align*}
    C_1A_1(gAg^{-1}-A_1) &= C_1(gAg^{-1})(gAg^{-1}) - C_1A_1^2 \\
    &= C_1(gA^2g^{-1} - A_1^2) \\
    &= 0, \text{ using } (\ref{eq:C_1(gA^ng^(-1)-A_1^n)=0}).
\end{align*}
Hence,
$$(gAg^{-1}-A_1)(X) \subset \ker(C_1) \cap \ker(C_1A_1).$$
Continuing in this way to use condition (\ref{eq:C_1(gA^ng^(-1)-A_1^n)=0}) inductively, we see that
\begin{align*}
\begin{split}
    (gAg^{-1}-A_1)(X) \subset \ker(C_1) \cap \ker(C_1A_1) \cap \ker(C_1A_1^2) \cap \dots = 0, \text{ by complete observability},
\end{split}
\end{align*}
which is of course just what is meant by (\ref{eq:A_1=gAg^(-1)}) (i.e. two linear maps are equal by definition if their difference maps every vector into zero).

Now, combine (\ref{eq:C=C_1g}) and (\ref{eq:A_1=gAg^(-1)}) with Theorem \ref{thm:Equivalence1}, i.e. the fact that the frequency responses are the same:
$$C(A-s)^{-1}B = C_1(A_1-s)^{-1}B_1$$
or
$$C_1g(A-s)^{-1}B = C_1(A_1-s)^{-1}B_1$$
or
$$C_1(A_1-s)^{-1}(gB) = C_1(A_1-s)^{-1}B_1.$$
Hence,
$$C_1(A_1^n)(gB-B_1) = 0 \text{ for all } n,$$
and complete observability now implies that
\begin{align} \label{eq:B_1=gB}
    B_1 = gB.
\end{align}
Relations (\ref{eq:A_1=gAg^(-1)}), (\ref{eq:C_1(gA^ng^(-1)-A_1^n)=0}) and (\ref{eq:B_1=gB}) are what are needed to show that $\sigma$ and $\sigma_1$ are algebraically equivalent, hence to finish the proof of Theorem \ref{thm:Equivalence2}.

Notice that complete controllability played no role in Theorem \ref{thm:Equivalence2}, because we assumed that the state spaces were the same.
If we drop this requirement, it does come in.

\begin{theorem} \label{thm:Equivalence3}
    Let $\sigma = (U, X, Y, A, B, C, D), \sigma_1 = (U, X_1, Y, A_1, B_1, C_1, D_1)$ be two linear systems, with the same input and output spaces, $U$ and $Y$, but different state spaces.
    Suppose that both $\sigma$ and $\sigma_1$ are completely controllable and observable, and that they are ``systems theoretically'' equivalent, in the sense that they have the same input and output pairs.
    Then, they are algebraically equivalent in the sense that there is an isomorphism
    $$g: X \to X_1$$
    such that:
    \begin{align}
        A_1 &= gAg^{-1} \\
        C &= C_1g \\
        C_1 &= Cg^{-1} \\
        B_1 &= gB.
    \end{align}
\end{theorem}

\begin{proof}
    Follow through the argument given in Theorem \ref{thm:Equivalence2}, and define $g$ again so that:
    $$x_1 = gx_0.$$
    (Now, $x_1 \in X_1, x_0 \in X_0$, but this makes no difference.)
    Again, (\ref{eq:CA^n=C_1A_1^ng}) is satisfied.
    Complete observability implies that:
    $$g \text{ is one-one}.$$
    By Theorem \ref{thm:Equivalence1},
    \begin{align} \label{eq:CA^nB=C_1A_1^nB_1}
        CA^nB &= C_1A_1^nB_1 \text{ for all } n.
    \end{align}
    Combine (\ref{eq:g(x_0)=x_1}) and (\ref{eq:CA^nB=C_1A_1^nB_1}):
    $$C_1A_1^ngB = C_1A_1^nB_1.$$
    Hence,
    $$C_1A_1^n(gB-B_1) = 0,$$
    or
    \begin{align} \label{eq:gB=B_1}
        gB &= B_1,
    \end{align}
    by complete observability again.
    Using (\ref{eq:g(x_0)=x_1}),
    \begin{align*}
    \begin{split}
        C_1(gA - A_1g) &= C_1gA - CA = 0, \text{ by } (\ref{eq:CA^n=C_1A_1^ng}) \\
        C_1A_1(gA - A_1g) &= CA^2 - CA_1^2g = 0, \text{ by } (\ref{eq:g(x_0)=x_1}).
    \end{split}
    \end{align*}
    Continuing in this way, we see that:
    $$C_1A_1^n(gA - A_1g) = 0 \text{ for all } n,$$
    hence, by complete observability,
    \begin{align} \label{eq:gA=A_1g}
        gA &= A_1g.
    \end{align}
    Having proved (\ref{eq:CA^n=C_1A_1^ng}), (\ref{eq:gB=B_1}) and (\ref{eq:gA=A_1g}) (using only observability), all that remains to be done is to prove $g^{-1}$ exists.
    Since $g$ is one-one, we must prove that $g$ is onto.
    
    Now, using (\ref{eq:gB=B_1}),
    \begin{align} \label{eq:B_1(U)subsetg(X)}
        B_1(U) \subset g(X).
    \end{align}
    
    $A_1B_1(U) = A_1gB(U)$, using (\ref{eq:B_1(U)subsetg(X)}), is equivalent to, using (\ref{eq:gA=A_1g}), $gAB(U) \subset g(X)$.
    Continuing in this way, we have:
    $$A_1^nB_1(U) \subset g(X) \text{ for all } n.$$
    Complete controllability of $\sigma_1$ now implies that
    $$g(X) = X_1,$$
    which finishes the proof of Theorem \ref{thm:Equivalence3}.
\end{proof}

\textbf{Corollary to Theorem \ref{thm:Equivalence3}}.
If two linear systems are completely controllable and observable, and are input-output equivalent, then their state spaces have the same dimension.

Later on, we shall see that there are ways of computing the dimension of the state space in terms of the frequency response function.

Let us turn to consideration of the ``duality'' between observability and controllability.

\begin{definition}
    Let $\sigma = (U, X, Y, A, B, C, D)$ be a linear system.
    Let:
    $$\sigma^d = (Y^d, X^d, U^d, A^d, C^d, B^d, D^d)$$
    be the dual algebraic object. Then, $\sigma^d$ is called the \textbf{dual system to} $\sigma$.
\end{definition}

Let us make more explicit what is meant by this.
Set:
$$\sigma^d = (U_1, X_1, Y_1, A_1, B_1, C_1, D_1),$$
i.e. $U_1$ is the input space of $\sigma^d$, $X_1$ is the state space, etc.
Here is the mapping diagram of $\sigma$.

\begin{equation*}
\begin{tikzcd} \label{diag:LinearSystem2}
    U \ar[r,"D"] \ar[d,"B"'] & Y \\
    X \ar[r,"A"'] & X \ar[u,"C"']
\end{tikzcd}
\end{equation*}

Apply the ``duality'' cofunctor to this diagram, i.e. replace each vector space by its dual vector space, and each map by its dual map.
We obtain the following mapping diagram:

\begin{equation*}
\begin{tikzcd} \label{diag:LinearSystem3}
    Y^d \ar[r,"D^d"] \ar[d,"C^d"'] & U^d \\
    X^d \ar[r,"A^d"'] & X^d \ar[u,"B^d"']
\end{tikzcd}
\end{equation*}

This is the diagram associated to the dual system $\sigma^d$.
Especially notice that the role of input and output spaces is interchanged under this duality.
For example, the dimension of the input space of $\sigma^d$ is the dimension of the output space of $\sigma$.
The state spaces of $\sigma$ and $\sigma^d$ are the same dimension, of course.

\begin{theorem} \label{thm:CompletelyObservableIffCompletelyControllable}
    $\sigma$ is completely observable if and only if $\sigma^d$ is completely controllable.
\end{theorem}

\begin{exercise}
    Prove Theorem \ref{thm:CompletelyObservableIffCompletelyControllable}.
\end{exercise}

Now, if a system $\sigma$ is not completely controllable, it is possible to reduce it to make a controllable one.
Here is a relevant

\begin{definition}
    Let $\sigma = (U, X, Y, A, B, C, D), \sigma_1 = (U_1, X_1, Y_1, A_1, B_1, C_1, D_1)$ be linear systems.
    Then, $\sigma_1$ is a \textbf{subsystem} of $\sigma$ if the following conditions are satisfied:
    \begin{align}
        U_1 &= U, Y_1 \\
        X_1 &\subset X \\
        D &= D_1.
    \end{align}
    The following diagram of maps are commutative:
    
    \begin{equation*}
    \begin{tikzcd}[column sep=tiny]
        X_1 \ar[d,"A_1"'] & \subset & X \ar[d,"A"] \\
        X_1 & \subset & X
    \end{tikzcd}
    \end{equation*}

    \begin{equation*}
    \begin{tikzcd}[column sep=tiny]
        X_1 \ar[d,"C_1"'] & \subset & X \ar[d,"C"] \\
        Y_1 & \subset & Y
    \end{tikzcd}
    \end{equation*}

    \begin{equation*}
    \begin{tikzcd}[column sep=tiny]
        U_1 \ar[d,"B_1"'] & \subset & U \ar[d,"B"] \\
        X_1 & \subset & X
    \end{tikzcd}
    \end{equation*}
\end{definition}

\begin{exercise}
    If $\sigma_1$ is a subsystem of $\sigma$, show that the set of input-output pairs of $\sigma_1$ is a subset of the input-output pairs of $\sigma$.
    Is the converse true?
\end{exercise}

\begin{exercise}
    If $\sigma_1$ is a subsystem of $\sigma$, and if $\sigma$ is completely observable, show that $\sigma_1$ is also completely observable.
\end{exercise}

\begin{theorem} \label{thm:CompletelyControllableSubsystem}
    Let $\sigma = (U, X, Y, A, B, C, D)$ be a system.
    Set:
    \begin{align} \label{eq:X_1=B(U)+AB(U)+A^2B(U)+...}
        X_1 &= B(U) + AB(U) + A^2B(U) + \dots
    \end{align}
    Then,
    \begin{align} \label{eq:A(X_1)subsetX_1}
        A(X_1) \subset X_1.
    \end{align}
    Set:
    \begin{align*}
    \begin{split}
        A_1 &= A \text{ restricted to } X_1 \\
        U_1 &= U, Y_1 = Y \\
        B_1(u) &= B(u) \text{ for } u \in U \\
        C_1 &= C \text{ restricted to } X_1 \\
        D_1 &= D.
    \end{split}
    \end{align*}
    Then, $\sigma_1 = (U, X_1, Y, A_1, B_1, C_1, D_1)$ is a completely controllable subsystem of $\sigma$.
\end{theorem}

\begin{proof}
    (\ref{eq:A(X_1)subsetX_1}) follows from (\ref{eq:X_1=B(U)+AB(U)+A^2B(U)+...}).
    All the rest is obvious.
\end{proof}

\begin{remark}
    This construction -- ``reducing'' the state space from $X$ to $X_1$, via formula (\ref{eq:A(X_1)subsetX_1}) -- provides us with a method of constructing a completely controllable \emph{and} observable system $\sigma_1$, starting with an observable one $\sigma$.
    We shall see that this method is useful later on in the ``realization theory.''
\end{remark}

To summarize, we have presented in this section some standard results and ideas -- none very difficult -- about linear, time-invariant systems, and their controllability and observability properties.

We may temporarily leave the study of linear systems in order to develop some elementary ideas of algebraic geometry.
While not all of this algebraic material will be immediately needed to study linear systems, I believe (along with Kalman) that it will ultimately prove relevant.

\chapter{Ideas of Algebraic Geometry}

\section{Introduction}

\section{Polynomials}

\section{Affine Algebraic Varieties}

\section{Projective Algebraic Varieties}

\section{Homogeneous and Inhomogeneous Coordinates}

\section{The Riemann Sphere and the Complex Projective Line}

\section{Linear Fractional and Projective Transformations}

\chapter{The Algebra of Polynomials}

\section{Introduction}

\section{Polynomials with Coefficients in a Ring}

\section{Integral Domains and Quotient Fields}

\section{The Algebra of Polynomials Over a Field}

\section{The Resultant of Two Polynomials}

\section{Resultant System of More Than Two Polynomials}

\chapter{General Properties of Affine and Projective Varieties}

\section{Introduction}

\section{Irreducible Varieties and Rational Functions}

\section{Dimension of Affine Varieties}

\section{The Zariski Topology}

\section{The Projective Embedding of the Grassmann Space}

\chapter{Linear Systems with Scalar Inputs and Outputs}

\section{Introduction}

\section{Hankel Maps and Matrices}

\section{The Algebraic Structure of Finitely Generated Modules Over $K[s]$}

\section{Cyclic Vectors and Complete Controllability for Scalar Input-Output Systems}

\section{Realization of Scalar Rational Functions as Frequency Response Functions}

\chapter{Rational Maps Between Vector Spaces and Linear Systems Defined Over General Fields}

\section{Introduction}

\section{Quotients of Modules Over Integral Domains}

\section{Rational Maps Between Vector Spaces}

\section{Linear Systems from the Standpoint of Rational Maps}

\section{Some Remarks on the Methodology of Generalization}

\chapter{The Realization of Rational Maps as Frequency Responses of Linear Systems}

\section{Introduction}

\section{Uniqueness of Realizations as Completely Controllable and Observable Systems}

\section{Existence of Realizations of Frequency Response Maps}

\section{Computation of the Dimension of the State Space of a Completely Controllable and Observable Realization}

\chapter{Algebraic Equivalence Relations and Orbit Spaces of Transformation Groups}

\section{Introduction}

\section{General Remarks About the ``Structure'' of Orbit Spaces}

\section{The Algebraic Equivalence Structure for Linear Systems}

\section{The Orbit Space for Systems with Scalar Inputs and Outputs}

% TODO: Add bibliography.

\end{document}
